{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'/home/zulissi/software/adamwr')\n",
    "import numpy as np\n",
    "import cgcnn\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 1\n",
    "import mongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# docs_all = pickle.load(open('/home/sback/work_dir/new_new_script/CO_final_adsorbate/CO_docs_connectivity.pkl','rb'))['docs']\n",
    "# SDT_list_relaxed=pickle.load(open('/home/sback/work_dir/new_new_script/CO_final_adsorbate/SDT_list_distance_relaxed.pkl','rb'))['SDT_list_distance_relaxed']\n",
    "# SDT_list_unrelaxed=pickle.load(open('/home/sback/work_dir/new_new_script/CO_final_adsorbate/SDT_list_distance_unrelaxed.pkl','rb')\n",
    "# )['SDT_list_distance_unrelaxed']\n",
    "\n",
    "# docs_all, SDT_list_relaxed, SDT_list_unrelaxed = zip(*[[doc,SDTR, SDTU] for doc,SDTR, SDTU \n",
    "#         in zip(docs_all, SDT_list_relaxed, SDT_list_unrelaxed) \n",
    "#         if -3.0 < doc['energy'] < 1.0])\n",
    "\n",
    "# target_list = np.array([doc['energy'] for doc in docs_all]).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(docs_all, open('/home/junwoony/Desktop/cgcnn/CO_20000_docs.pkl', 'wb'))\n",
    "# pickle.dump(SDT_list_unrelaxed, open('/home/junwoony/Desktop/cgcnn/CO_20000_SDT_list_unrelaxed.pkl', 'wb'))\n",
    "# pickle.dump(SDT_list_relaxed, open('/home/junwoony/Desktop/cgcnn/CO_20000_SDT_list_relaxed.pkl', 'wb'))\n",
    "# pickle.dump(target_list, open('/home/junwoony/Desktop/cgcnn/CO_20000_target_list.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# docs_all = pickle.load(open('/home/junwoony/Desktop/cgcnn/CO_20000_docs.pkl', 'rb'))\n",
    "# SDT_list_unrelaxed = pickle.load(open('/home/junwoony/Desktop/cgcnn/CO_20000_SDT_list_unrelaxed.pkl', 'rb'))\n",
    "# structures = SDT_list_unrelaxed[0]\n",
    "# # SDT_list_relaxed = pickle.load(open('/home/junwoony/Desktop/cgcnn/CO_20000_SDT_list_relaxed.pkl', 'rb'))\n",
    "# # structures = SDT_list_relaxed[0]\n",
    "\n",
    "# target_list = pickle.load(open('/home/junwoony/Desktop/cgcnn/CO_20000_target_list.pkl', 'rb'))\n",
    "\n",
    "# #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "# orig_atom_fea_len = structures[0].shape[-1]\n",
    "# nbr_fea_len = structures[1].shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import mongo\n",
    "# from cgcnn.data_JY import StructureData, ListDataset, StructureDataTransformer\n",
    "# import numpy as np\n",
    "# import tqdm\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# SDT = StructureDataTransformer(atom_init_loc='/home/zulissi/software/cgcnn_sklearn/atom_init.json',\n",
    "#                               max_num_nbr=12,\n",
    "#                                step=0.2,\n",
    "#                               radius=1,\n",
    "#                               use_tag=False,\n",
    "#                               use_fixed_info=False,\n",
    "#                               use_distance=True,\n",
    "#                               train_geometry = 'final-adsorbate'\n",
    "#                               )\n",
    "\n",
    "# SDT_out = SDT.transform(docs_all)\n",
    "\n",
    "# structures = SDT_out[0]\n",
    "\n",
    "# #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "# orig_atom_fea_len = structures[0].shape[-1]\n",
    "# nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "# import multiprocess as mp\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# SDT_out = SDT.transform(docs_all)\n",
    "\n",
    "# with mp.Pool(4) as pool:\n",
    "#     SDT_list_unrelaxed = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "\n",
    "# from cgcnn.data import collate_pool, MergeDataset\n",
    "# from cgcnn.model import CrystalGraphConvNet\n",
    "\n",
    "from cgcnn.data_JY import collate_pool, MergeDataset\n",
    "from cgcnn.model_JY import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# indices = np.arange(len(SDT_list_unrelaxed))\n",
    "# SDT_training, SDT_test, target_training, target_test, train_idx, test_idx \\\n",
    "# = train_test_split(SDT_list_unrelaxed, target_list, indices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(docs_all, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_docs.pkl', 'wb'))\n",
    "# pickle.dump(SDT_list_unrelaxed, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_list_unrelaxed.pkl', 'wb'))\n",
    "# pickle.dump(target_list, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_list_unrelaxed.pkl', 'wb'))\n",
    "\n",
    "# pickle.dump(SDT_training, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_training.pkl', 'wb'))\n",
    "# pickle.dump(SDT_test, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_test.pkl', 'wb'))\n",
    "# pickle.dump(target_training, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_training.pkl', 'wb'))\n",
    "# pickle.dump(target_test, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_test.pkl', 'wb'))\n",
    "# pickle.dump(train_idx, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_train_idx.pkl', 'wb'))\n",
    "# pickle.dump(test_idx, open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_test_idx.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "docs = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_docs.pkl', 'rb'))\n",
    "SDT_list_unrelaxed = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_list_unrelaxed.pkl', 'rb'))\n",
    "target_list = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_list_unrelaxed.pkl', 'rb'))\n",
    "\n",
    "SDT_training = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_training.pkl', 'rb'))\n",
    "SDT_test = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_SDT_test.pkl', 'rb'))\n",
    "target_training = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_training.pkl', 'rb'))\n",
    "target_test = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_target_test.pkl', 'rb'))\n",
    "train_idx = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_train_idx.pkl', 'rb'))\n",
    "test_idx = pickle.load(open('/home/junwoony/Desktop/cgcnn/cgcnn_vis/CO_20000_test_idx.pkl', 'rb'))\n",
    "\n",
    "structures = SDT_training[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamw import AdamW\n",
    "from cosine_scheduler import CosineLRWithRestarts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "\n",
    "# warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=214, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "\n",
    "#############\n",
    "# To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):\n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        return super().get_loss(y_pred, y_true, **kwargs)\n",
    "## return features = net.forward(SDT_test)\n",
    "############\n",
    "\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs= 200, \n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=6, #8\n",
    "    module__n_h=2,\n",
    "    optimizer__weight_decay=1e-5,\n",
    "    optimizer=AdamW,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.fit(SDT_training, target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.initialize()\n",
    "# net.load_params(f_history='valid_best_history.json',\n",
    "#                 f_optimizer= 'valid_best_optimizer.pt', \n",
    "#                 f_params='valid_best_params.pt'\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "\n",
    "training_data = {'actual_value':target_training[train_indices].reshape(-1),\n",
    "                 'predicted_value':net.predict(SDT_training)[train_indices].reshape(-1)}\n",
    "validation_data = {'actual_value':target_training[valid_indices].reshape(-1),\n",
    "                 'predicted_value':net.predict(SDT_training)[valid_indices].reshape(-1)}\n",
    "test_data = {'actual_value':target_test.reshape(-1),\n",
    "                 'predicted_value':net.predict(SDT_test).reshape(-1)}\n",
    "\n",
    "df_training = pd.DataFrame(training_data)\n",
    "df_validation = pd.DataFrame(validation_data)\n",
    "df_test = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training.to_csv('training.csv', sep='\\t', index=True)\n",
    "# df_test.to_csv('test.csv', sep='\\t', index=True)\n",
    "# df_validation.to_csv('validation.csv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.scatter(df_training['actual_value'], df_training['predicted_value'], color='orange', \n",
    "           marker='o', alpha=0.5, label='train\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_training['actual_value'], df_training['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_training['actual_value'], df_training['predicted_value'])),\n",
    "              r2_score(df_training['actual_value'], df_training['predicted_value'])))\n",
    "\n",
    "\n",
    "ax.scatter(df_validation['actual_value'], df_validation['predicted_value'], color='blue', \n",
    "           marker='o', alpha=0.5, label='valid\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_validation['actual_value'], df_validation['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_validation['actual_value'], df_validation['predicted_value'])),\n",
    "              r2_score(df_validation['actual_value'], df_validation['predicted_value'])))\n",
    "\n",
    "ax.scatter(df_test['actual_value'], df_test['predicted_value'], color='green', \n",
    "           marker='o', alpha=0.5, label='test\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_test['actual_value'], df_test['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_test['actual_value'], df_test['predicted_value'])),\n",
    "              r2_score(df_test['actual_value'], df_test['predicted_value'])))\n",
    "\n",
    "\n",
    "ax.plot([min(df_training['actual_value']), max(df_training['actual_value'])], \n",
    "        [min(df_training['actual_value']), max(df_training['actual_value'])], 'k--')\n",
    "\n",
    "# format graph\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('DFT E (eV)', fontsize=14)\n",
    "ax.set_ylabel('CGCNN predicted E (eV)', fontsize=14)\n",
    "ax.set_title('Multi-element ', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "plt.savefig('prediction.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of accurate predictions\n",
    "\n",
    "# visual_idx = np.where(abs(test_data['actual_value'] - test_data['predicted_value']) < 0.0001)[0]\n",
    "visual_idx = np.where(abs(training_data['actual_value'] - training_data['predicted_value']) < 0.0001)[0]\n",
    "\n",
    "for idx in visual_idx:\n",
    "    out, atom_fea_vis, atom_fea = net.forward([SDT_training[idx]])\n",
    "    contributions = atom_fea_vis.cpu().data.numpy().reshape(-1)\n",
    "    # Get idx from docs\n",
    "    doc_test_idx = train_idx[idx]\n",
    "    atoms = mongo.make_atoms_from_doc(docs[doc_test_idx])\n",
    "    atoms.set_initial_charges(contributions)\n",
    "    atoms.write('./traj/%d.traj'%(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('atom feature before considering connectivity \\n',atom_fea.reshape(-1),'\\n')\n",
    "print('atom feature after considering connectivity \\n', atom_fea_vis.reshape(-1),'\\n')\n",
    "print('contributions of atoms \\n', contributions,'\\n')\n",
    "print('mean of the contributions \\n', torch.sum(atom_fea_vis.reshape(-1))/len(torch.nonzero(atom_fea_vis.reshape(-1))),'\\n')\n",
    "print('prediction (must be equal to the mean above) \\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

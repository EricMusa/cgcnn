{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zulissi/miniconda3/envs/cgcnn/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/home/zulissi/miniconda3/envs/cgcnn/lib/python3.7/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "# sys.path.insert(0,'/home/zulissi/software/adamwr')\n",
    "import numpy as np\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "# from torchviz import make_dot, make_dot_from_trace\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cgcnn.data_grad import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import multiprocess as mp\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4071/4071 [03:29<00:00, 19.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDT conversion time 216.69766068458557\n",
      "SDT list created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%cache cached_docs.pkl docs SDT_list  orig_atom_fea_len nbr_fea_len\n",
    "docs = pickle.load(open('../input/H_docs.pkl', 'rb'))\n",
    "\n",
    "docs = [doc for doc in docs if doc['coordination']==doc['initial_coordination'] and doc['coordination']!=''\n",
    "                               and doc['movement_data']['max_adsorbate_movement']<1.0\n",
    "                               and doc['movement_data']['max_slab_movement']<0.2\n",
    "                               and 'N' not in doc['atoms']['chemical_symbols'] #no nitrides\n",
    "                               and 'C' not in doc['atoms']['chemical_symbols'] #no carbides\n",
    "                               and 'S' not in doc['atoms']['chemical_symbols'] #no sulfides\n",
    "                               and 'Se' not in doc['atoms']['chemical_symbols'] #no selenides\n",
    "                               and doc['atoms']['symbol_counts']['H']<2 #no hydrides\n",
    "        ]\n",
    "random.shuffle(docs)\n",
    "docs = docs[:]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "SDT = StructureDataTransformer(atom_init_loc='../atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step= 0.5,\n",
    "                              radius=12,\n",
    "                              use_tag=True,\n",
    "                               rattle=0.0,\n",
    "                              use_fixed_info=True,\n",
    "                              bond_property=True,\n",
    "#                               use_voronoi=False,\n",
    "#                               train_geometry = 'initial',\n",
    "                              is_initial=True\n",
    "                              )\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "#     SDT_list = list(tqdm.tqdm(map(lambda x: SDT_out[x],range(len(SDT_out))),total=len(SDT_out)))\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "\n",
    "structures = SDT_list[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "SDT_time = end-start\n",
    "print('SDT conversion time', SDT_time)\n",
    "print('SDT list created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = np.array([sdt[-1].numpy() for sdt in SDT_list]).reshape(-1,1)\n",
    "# pickle.dump(SDT_list, open('./input/SDT_20000_grad.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "\n",
    "from cgcnn.data_grad import collate_pool, MergeDataset\n",
    "from cgcnn.model_grad import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SDT_list= SDT_list\n",
    "target_list = target_list\n",
    "\n",
    "indices = np.arange(len(SDT_list))\n",
    "SDT_training, SDT_test, target_training, target_test, train_idx, test_idx \\\n",
    "= train_test_split(SDT_list, target_list, indices, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get an idea for what the DFT initial - final structure difference is; the ML models needs to beat this number for the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0842243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff(target,sdt):\n",
    "    free_atom_idx = list(set(list(range(len(target[0]))))-set(sdt[-2].numpy()))\n",
    "    diff = np.sum(((target[0] - sdt[4].numpy())[free_atom_idx])**2.,axis=1)**0.5\n",
    "#     diff = (target[0] - sdt[4].numpy())[free_atom_idx]\n",
    "\n",
    "    return diff\n",
    "\n",
    "np.mean(np.abs(np.concatenate([diff(target,sdt) for sdt,target in zip(SDT_test, target_test)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamwr.adamw import AdamW\n",
    "from torch.optim.lbfgs import LBFGS\n",
    "\n",
    "from adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "\n",
    "batch_size=40\n",
    "\n",
    "# warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=batch_size, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "\n",
    "\n",
    "#############\n",
    "# To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):        \n",
    "#         pos_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        pos_pred = y_pred[0]\n",
    "        additional_loss = y_pred[1]\n",
    "        return torch.mean(torch.sqrt(torch.sum((pos_pred-y_true.cuda())**2.0,dim=1)))*10 \n",
    "#         return super().get_loss(y_pred, y_true, **kwargs)\n",
    "# return features = net.forward(SDT_test)\n",
    "############\n",
    "\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=batch_size, #214\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs= 1000, \n",
    "    module__atom_fea_len=46, #46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8, #8\n",
    "    module__n_h=8,\n",
    "        module__opt_step_size=0.3,\n",
    "    module__min_opt_steps=20,\n",
    "    module__max_opt_steps=300,\n",
    "    module__step=0.5,\n",
    "    module__dmax=12,\n",
    "    optimizer__weight_decay=1e-3,\n",
    "    optimizer=AdamW,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, LR_schedule, load_best_valid_loss] #    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, dmax, h_fea_len, max_opt_steps, min_opt_steps, n_conv, n_h, nbr_fea_len, opt_step_size, orig_atom_fea_len, step.\n",
      "Re-initializing optimizer because the following parameters were re-set: weight_decay.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m1.7940\u001b[0m        \u001b[32m1.6691\u001b[0m     +  30.7990\n",
      "      2        \u001b[36m0.9251\u001b[0m        \u001b[32m0.8597\u001b[0m     +  23.0604\n",
      "      3        \u001b[36m0.8611\u001b[0m        \u001b[32m0.8245\u001b[0m     +  21.8266\n",
      "      4        \u001b[36m0.8027\u001b[0m        \u001b[32m0.8045\u001b[0m     +  22.1534\n",
      "      5        \u001b[36m0.7838\u001b[0m        \u001b[32m0.7751\u001b[0m     +  22.6400\n",
      "      6        \u001b[36m0.7719\u001b[0m        \u001b[32m0.7647\u001b[0m     +  22.5658\n",
      "      7        \u001b[36m0.7480\u001b[0m        \u001b[32m0.7610\u001b[0m     +  22.7132\n",
      "      8        0.7759        \u001b[32m0.7591\u001b[0m     +  22.9084\n",
      "      9        \u001b[36m0.7479\u001b[0m        0.7625        23.0067\n",
      "     10        0.7680        \u001b[32m0.7572\u001b[0m     +  22.7852\n",
      "     11        0.7526        0.7757        49.5892\n",
      "     12        0.7540        0.7664        50.5211\n",
      "     13        0.7554        \u001b[32m0.7545\u001b[0m     +  49.1447\n",
      "     14        \u001b[36m0.7244\u001b[0m        \u001b[32m0.7423\u001b[0m     +  49.1398\n",
      "     15        0.7565        \u001b[32m0.7406\u001b[0m     +  50.7508\n",
      "     16        0.7393        \u001b[32m0.7400\u001b[0m     +  50.5194\n",
      "     17        0.7588        \u001b[32m0.7374\u001b[0m     +  49.7730\n",
      "     18        \u001b[36m0.7242\u001b[0m        \u001b[32m0.7341\u001b[0m     +  50.6244\n",
      "     19        \u001b[36m0.7194\u001b[0m        \u001b[32m0.7333\u001b[0m     +  50.4226\n",
      "     20        0.7529        0.7353        50.4726\n",
      "     21        0.7340        0.7663        50.6379\n",
      "     22        0.7548        0.8020        50.1969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.MyNet'>[initialized](\n",
       "  module_=CrystalGraphConvNet(\n",
       "    (embedding): Linear(in_features=94, out_features=46, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (2): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (3): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (4): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (5): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (6): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (7): ConvLayer(\n",
       "        (fc_full): Linear(in_features=117, out_features=92, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "        (softplus1): LeakyReLU(negative_slope=0.01)\n",
       "        (bn1): BatchNorm1d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (softplus2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "    )\n",
       "    (conv_to_fc): Linear(in_features=117, out_features=83, bias=True)\n",
       "    (conv_to_fc_softplus): LeakyReLU(negative_slope=0.01)\n",
       "    (dist_fcs): ModuleList(\n",
       "      (0): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (1): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (2): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (3): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (4): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (5): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (6): Linear(in_features=117, out_features=117, bias=True)\n",
       "    )\n",
       "    (dist_softpluses): ModuleList(\n",
       "      (0): Sigmoid()\n",
       "      (1): Sigmoid()\n",
       "      (2): Sigmoid()\n",
       "      (3): Sigmoid()\n",
       "      (4): Sigmoid()\n",
       "      (5): Sigmoid()\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "    (dist_bn): ModuleList(\n",
       "      (0): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): BatchNorm1d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (const_fcs): ModuleList(\n",
       "      (0): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (1): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (2): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (3): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (4): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (5): Linear(in_features=117, out_features=117, bias=True)\n",
       "      (6): Linear(in_features=117, out_features=117, bias=True)\n",
       "    )\n",
       "    (const_softpluses): ModuleList(\n",
       "      (0): Sigmoid()\n",
       "      (1): Sigmoid()\n",
       "      (2): Sigmoid()\n",
       "      (3): Sigmoid()\n",
       "      (4): Sigmoid()\n",
       "      (5): Sigmoid()\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "    (const_bn): ModuleList(\n",
       "      (0): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc_to_bond_distance): Linear(in_features=117, out_features=1, bias=True)\n",
       "    (bond_distance_softplus): Softplus(beta=1, threshold=20)\n",
       "    (fc_to_bond_constant): Linear(in_features=117, out_features=1, bias=True)\n",
       "    (bond_const_softplus): Softplus(beta=1, threshold=20)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "# net.module_.max_opt_steps=40\n",
    "# net.max_epochs=400\n",
    "# net.module_.update_spring=100\n",
    "# net.module_.variable_bond_constant=False\n",
    "# net.partial_fit(SDT_training, target_training)\n",
    "net.module_.max_opt_steps=100\n",
    "net.max_epochs=10\n",
    "net.module_.update_spring=1000\n",
    "net.module_.variable_bond_constant=False\n",
    "net.partial_fit(SDT_training, target_training)\n",
    "net.module_.max_opt_steps=100\n",
    "net.max_epochs=100\n",
    "net.module_.update_spring=1000\n",
    "net.module_.variable_bond_constant=True\n",
    "net.partial_fit(SDT_training, target_training)\n",
    "# net.module_.max_opt_steps=100\n",
    "# net.max_epochs=1000\n",
    "# net.module_.update_spring=10\n",
    "# net.module_.variable_bond_constant=True\n",
    "# net.partial_fit(SDT_training, target_training)\n",
    "# net.module_.max_opt_steps=50\n",
    "# net.module_.max_epochs=100\n",
    "# net.module_.update_spring=10\n",
    "# net.partial_fit(SDT_training, target_training)\n",
    "# net.module_.min_opt_steps=300\n",
    "# net.module_.max_epochs=10\n",
    "# net.partial_fit(SDT_training, target_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ase.io\n",
    "import mongo\n",
    "mean_error_ml = []\n",
    "mean_error_dft = []\n",
    "net.load_params('valid_best_params.pt')\n",
    "\n",
    "with ase.io.trajectory.TrajectoryWriter('test.traj') as traj:\n",
    "    for i, (doc, sdt) in enumerate(zip(np.array(docs)[test_idx], SDT_test)):\n",
    "        atoms_final = mongo.make_atoms_from_doc(doc)\n",
    "        final_pos = sdt[-1]\n",
    "        atoms_final.positions=final_pos\n",
    "        atoms_init = mongo.make_atoms_from_doc(doc['initial_configuration'])\n",
    "        ml_pos = list(net.forward_iter([sdt]))[0][-1].detach().numpy()\n",
    "        atoms_ml = atoms_final.copy()\n",
    "        atoms_ml.positions = ml_pos\n",
    "        traj.write(atoms_init)\n",
    "        traj.write(atoms_ml)\n",
    "        traj.write(atoms_final)\n",
    "        diff = np.abs(np.sqrt(np.sum((ml_pos-final_pos.numpy())**2.,axis=1)))\n",
    "#         diff = diff[diff>1e-6]\n",
    "        \n",
    "        free_indices = list(set(range(len(atoms_final)))-set(atoms_final.constraints[0].get_indices()))\n",
    "        diff = diff[free_indices]\n",
    "        \n",
    "        mean_error_ml.append(diff)\n",
    "        \n",
    "        diff = np.abs(np.sqrt(np.sum((atoms_init.positions-final_pos.numpy())**2.,axis=1)))\n",
    "        diff = diff[free_indices]\n",
    "        #diff = diff[diff>1e-6]\n",
    "        mean_error_dft.append(np.abs(diff))\n",
    "\n",
    "mean_error_ml = np.array(mean_error_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00699114, 0.02874953, 0.02458825, 0.00727477, 0.01343165,\n",
       "       0.01303214, 0.01296901, 0.01335833, 0.1508423 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "124*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([387,  58, 763, 663, 488, 134, 221,  51, 476, 337, 604, 233, 680,\n",
       "       719, 142, 294, 696, 751, 814, 413, 512, 644, 760, 179, 211, 242,\n",
       "       772, 463, 234, 202, 588, 560, 110, 753,  56, 721,  74, 264,  71,\n",
       "       163, 666, 339,  41, 305, 426, 114, 653, 407,  96, 460,  17, 489,\n",
       "       188, 728, 225, 236, 771, 373,  66, 768, 541, 693, 516, 219,  99,\n",
       "       213, 701, 218, 673, 639, 143, 674, 440, 164, 678, 399, 120, 809,\n",
       "       565, 333, 623, 408, 465, 425, 734,  69,  98, 608, 749,  65, 527,\n",
       "       246, 549, 137, 567, 583, 713, 168, 435, 543, 104, 403,  37, 232,\n",
       "        87, 276, 102, 184, 295, 506, 646, 661, 535, 479, 759, 794, 365,\n",
       "        62, 171, 450, 609,   2, 722, 752,  23, 343, 124, 438, 473, 411,\n",
       "       540, 203, 566, 625,  49, 453, 544, 594,  29, 345, 240, 227, 158,\n",
       "       584, 216, 138, 791, 804, 743, 384, 368,  35, 351, 747, 297,  54,\n",
       "       378, 611, 322,   1, 350, 393, 359, 429, 769, 358, 554, 485, 132,\n",
       "         7,  95, 421, 430, 546, 746, 707, 483, 291,  85,   0, 354, 725,\n",
       "       287, 380, 732, 635, 318,  33, 441, 676,  97, 480, 223, 699,  68,\n",
       "       669, 787, 668, 513, 466,   4,  36, 406, 630, 263, 433,  16, 313,\n",
       "       552, 758,  21, 602,  93, 522, 363, 793, 328, 402, 101,  83, 392,\n",
       "       688, 336, 756,  82, 197, 671, 298, 735,   6, 180, 247, 262, 397,\n",
       "       470, 662, 361, 178, 235, 762, 442, 655, 374, 605,  48, 165, 383,\n",
       "       627, 170, 478,  61,   9, 356,  11, 775, 299,  84, 265, 155,  10,\n",
       "       754, 675,  14, 195, 492, 245, 576, 389, 621, 177, 486,  28, 776,\n",
       "       231, 355, 698, 723, 520,  70,  59, 434, 286, 391,  92, 487, 658,\n",
       "       205, 125, 340, 272,  45, 700, 173, 117, 400, 764, 230, 795, 409,\n",
       "       665, 196, 733, 536, 127,  55, 362,   3, 238, 755, 477, 471, 273,\n",
       "       111, 319, 268, 204, 619, 153, 214, 577,  19, 726, 715, 452, 308,\n",
       "        40, 316, 193, 631, 458, 598, 657, 741, 174, 206, 563, 150, 780,\n",
       "       217, 321, 192, 559, 432, 774, 182, 498, 183, 140, 528, 785, 539,\n",
       "       750, 451, 738, 109, 786,  30, 538, 503, 310, 190, 251, 614, 634,\n",
       "       210, 314, 507, 706, 189, 667, 275, 269, 459, 617,  50, 405, 593,\n",
       "       784, 311, 446,  32, 237, 717, 229, 280, 176, 431, 377,  80, 493,\n",
       "       716, 599, 300, 578, 360, 526, 482, 782, 274,  76, 467, 419, 187,\n",
       "       439, 537, 800, 181, 628, 695, 398, 606, 690,  31, 447,  38, 515,\n",
       "         5, 664, 250, 468,  91, 327, 500, 385, 271, 799, 103, 166, 292,\n",
       "       220, 697, 510, 778,  89, 370, 765, 797, 255, 257, 685, 320, 461,\n",
       "       371, 364, 684, 495, 445, 427, 802, 106, 729, 519, 810, 545, 744,\n",
       "       710, 574, 449, 331, 801, 289, 582, 633, 207, 209, 659, 334, 112,\n",
       "       259, 302, 557, 803, 581, 681, 692, 122, 326, 796,  77, 285, 649,\n",
       "       267, 159, 525, 523, 650, 191, 215, 534, 562,  46,  42, 147, 123,\n",
       "       288, 175, 591, 805, 529, 156, 404, 731, 348, 709, 777, 418, 382,\n",
       "       607, 603, 645,  88, 252, 508, 118, 423, 548, 491, 711, 457, 761,\n",
       "       556, 573, 505, 347, 691, 372, 511, 514, 415, 808, 330, 679, 341,\n",
       "       677, 284, 469, 129, 654, 481, 530, 396, 475, 151, 315, 160, 357,\n",
       "       587,  67, 198, 561,  86, 353, 105, 647, 656, 448, 736, 312, 278,\n",
       "       130, 304, 436, 148, 172, 293, 773,  15, 462, 748, 807, 670, 366,\n",
       "       472, 686, 464, 643,  12, 798, 131, 222, 564, 243,  26, 108, 253,\n",
       "       126, 813,  63, 144, 226, 496, 553, 597, 119,  64,  20,  79, 497,\n",
       "       682, 379, 766, 344, 428, 589, 620,  13, 641, 616, 558, 767, 694,\n",
       "       375, 612, 283, 703, 185,  27, 303, 306, 806, 727, 386, 239, 524,\n",
       "       244, 270, 133, 266, 712, 152, 569, 811, 100, 113, 317, 121, 412,\n",
       "       157, 417, 416, 224,  53, 296, 414, 260, 571, 740, 600, 615, 309,\n",
       "       381, 596, 651,  57, 757, 551, 788, 501, 332, 352, 789, 422, 550,\n",
       "       146, 636, 730,  78, 745, 248, 622,  94, 642,  34, 474, 338, 555,\n",
       "       208, 162, 186, 323, 637, 783, 277, 504, 329, 437, 154, 254, 579,\n",
       "       201, 718, 261, 115, 632, 640, 301, 443, 704, 590,   8, 367, 194,\n",
       "       638, 502, 249, 812, 161,  18, 610, 648, 570, 683,  90, 241, 575,\n",
       "       349, 484, 629, 136,  43, 499, 135, 490, 394, 212, 116, 424, 281,\n",
       "        52, 714, 580,  75,  47, 708,  60, 169, 509, 455, 720, 521, 770,\n",
       "       376, 724, 652,  25,  72, 454, 149, 572, 420, 737, 390, 739, 279,\n",
       "       199,  44, 324,  22, 781, 258, 346, 369, 568, 585,  73, 618, 687,\n",
       "       444, 592, 342, 325, 689, 107, 702, 542, 742, 167, 290,  39, 388,\n",
       "       792, 533, 547, 139, 517, 307, 705, 613, 335, 141, 779, 626, 395,\n",
       "       586,  24, 595, 401, 601, 128, 660, 200, 410, 282,  81, 532, 145,\n",
       "       494, 256, 624, 790, 672, 456, 518, 228, 531])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([np.mean(a) for a in mean_error_ml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010505209,\n",
       " 0.012501226,\n",
       " 0.013323843,\n",
       " 0.021876266,\n",
       " 0.023562385,\n",
       " 0.026124582,\n",
       " 0.026549827,\n",
       " 0.027632609,\n",
       " 0.02771201,\n",
       " 0.027791156,\n",
       " 0.027810656,\n",
       " 0.027891183,\n",
       " 0.027954396,\n",
       " 0.027956123,\n",
       " 0.028427042,\n",
       " 0.029174507,\n",
       " 0.029490754,\n",
       " 0.030021798,\n",
       " 0.03013756,\n",
       " 0.030419713,\n",
       " 0.030927833,\n",
       " 0.031121887,\n",
       " 0.031503823,\n",
       " 0.031510875,\n",
       " 0.031827495,\n",
       " 0.032050446,\n",
       " 0.03219772,\n",
       " 0.032501884,\n",
       " 0.032531083,\n",
       " 0.032778922,\n",
       " 0.03298989,\n",
       " 0.03476367,\n",
       " 0.035927042,\n",
       " 0.036295604,\n",
       " 0.036836136,\n",
       " 0.037446383,\n",
       " 0.037715245,\n",
       " 0.03782248,\n",
       " 0.037954744,\n",
       " 0.03817786,\n",
       " 0.038178813,\n",
       " 0.038433716,\n",
       " 0.03846635,\n",
       " 0.038613856,\n",
       " 0.0394441,\n",
       " 0.03947965,\n",
       " 0.03979223,\n",
       " 0.039926358,\n",
       " 0.040320847,\n",
       " 0.04079526,\n",
       " 0.040944234,\n",
       " 0.04101831,\n",
       " 0.041208692,\n",
       " 0.041290663,\n",
       " 0.041390333,\n",
       " 0.04188499,\n",
       " 0.04191116,\n",
       " 0.042026557,\n",
       " 0.04206192,\n",
       " 0.042127833,\n",
       " 0.04257621,\n",
       " 0.04257992,\n",
       " 0.042684853,\n",
       " 0.042882875,\n",
       " 0.042990677,\n",
       " 0.043232314,\n",
       " 0.043687746,\n",
       " 0.043780442,\n",
       " 0.043791424,\n",
       " 0.043942742,\n",
       " 0.04396769,\n",
       " 0.044022888,\n",
       " 0.04438492,\n",
       " 0.04444545,\n",
       " 0.04445173,\n",
       " 0.044935383,\n",
       " 0.045112666,\n",
       " 0.04571541,\n",
       " 0.045723397,\n",
       " 0.045779254,\n",
       " 0.045794737,\n",
       " 0.04580307,\n",
       " 0.04586612,\n",
       " 0.04613311,\n",
       " 0.04620679,\n",
       " 0.04631601,\n",
       " 0.046536896,\n",
       " 0.04657296,\n",
       " 0.04666157,\n",
       " 0.0466905,\n",
       " 0.046882026,\n",
       " 0.046883233,\n",
       " 0.046884477,\n",
       " 0.047015805,\n",
       " 0.047058877,\n",
       " 0.047061082,\n",
       " 0.047242843,\n",
       " 0.04725054,\n",
       " 0.047622096,\n",
       " 0.047881506,\n",
       " 0.047952,\n",
       " 0.04805369,\n",
       " 0.048104968,\n",
       " 0.04817126,\n",
       " 0.04818152,\n",
       " 0.048291843,\n",
       " 0.048436347,\n",
       " 0.04846507,\n",
       " 0.04866913,\n",
       " 0.04877361,\n",
       " 0.048870128,\n",
       " 0.048948657,\n",
       " 0.04933969,\n",
       " 0.049417302,\n",
       " 0.049450323,\n",
       " 0.04947955,\n",
       " 0.049556334,\n",
       " 0.049642984,\n",
       " 0.049957424,\n",
       " 0.05007102,\n",
       " 0.050095614,\n",
       " 0.05034449,\n",
       " 0.050367955,\n",
       " 0.050379556,\n",
       " 0.050379783,\n",
       " 0.05039042,\n",
       " 0.050496466,\n",
       " 0.05062613,\n",
       " 0.050885454,\n",
       " 0.051024597,\n",
       " 0.051049065,\n",
       " 0.0512469,\n",
       " 0.05132379,\n",
       " 0.05136679,\n",
       " 0.05139817,\n",
       " 0.051506724,\n",
       " 0.05157145,\n",
       " 0.051600534,\n",
       " 0.05175025,\n",
       " 0.05191132,\n",
       " 0.051975325,\n",
       " 0.052218914,\n",
       " 0.052344702,\n",
       " 0.052710954,\n",
       " 0.052842826,\n",
       " 0.05288404,\n",
       " 0.05325796,\n",
       " 0.053260066,\n",
       " 0.053267695,\n",
       " 0.053358797,\n",
       " 0.053716928,\n",
       " 0.05380623,\n",
       " 0.05397323,\n",
       " 0.054039966,\n",
       " 0.05411611,\n",
       " 0.054157972,\n",
       " 0.05416637,\n",
       " 0.054217555,\n",
       " 0.05422566,\n",
       " 0.054404404,\n",
       " 0.05447166,\n",
       " 0.05458574,\n",
       " 0.054823093,\n",
       " 0.05505038,\n",
       " 0.055138525,\n",
       " 0.055230204,\n",
       " 0.055355746,\n",
       " 0.055564895,\n",
       " 0.05575501,\n",
       " 0.055765085,\n",
       " 0.056174643,\n",
       " 0.056413166,\n",
       " 0.05644171,\n",
       " 0.05647129,\n",
       " 0.05676296,\n",
       " 0.056823265,\n",
       " 0.057232216,\n",
       " 0.057565384,\n",
       " 0.057654772,\n",
       " 0.057740573,\n",
       " 0.057776295,\n",
       " 0.05791157,\n",
       " 0.058109753,\n",
       " 0.058165733,\n",
       " 0.05819039,\n",
       " 0.058277342,\n",
       " 0.058309853,\n",
       " 0.058435783,\n",
       " 0.058436476,\n",
       " 0.058467615,\n",
       " 0.05852053,\n",
       " 0.058567658,\n",
       " 0.05859387,\n",
       " 0.058970783,\n",
       " 0.059101224,\n",
       " 0.059235327,\n",
       " 0.05930214,\n",
       " 0.059488654,\n",
       " 0.059744306,\n",
       " 0.059771433,\n",
       " 0.05989789,\n",
       " 0.059918024,\n",
       " 0.060274478,\n",
       " 0.06027799,\n",
       " 0.060428035,\n",
       " 0.060534373,\n",
       " 0.060622722,\n",
       " 0.060679864,\n",
       " 0.06076928,\n",
       " 0.060776744,\n",
       " 0.06082125,\n",
       " 0.060962845,\n",
       " 0.061001047,\n",
       " 0.061130956,\n",
       " 0.0611322,\n",
       " 0.061351873,\n",
       " 0.06141093,\n",
       " 0.061545122,\n",
       " 0.061891597,\n",
       " 0.06203054,\n",
       " 0.06218316,\n",
       " 0.06232725,\n",
       " 0.06238749,\n",
       " 0.06254404,\n",
       " 0.06264172,\n",
       " 0.062661506,\n",
       " 0.06266469,\n",
       " 0.06279848,\n",
       " 0.06328104,\n",
       " 0.064026624,\n",
       " 0.06424966,\n",
       " 0.06475555,\n",
       " 0.06521779,\n",
       " 0.06528739,\n",
       " 0.06533949,\n",
       " 0.06540749,\n",
       " 0.06542169,\n",
       " 0.06568193,\n",
       " 0.06577836,\n",
       " 0.06588895,\n",
       " 0.066031314,\n",
       " 0.0661459,\n",
       " 0.06634802,\n",
       " 0.06639631,\n",
       " 0.06704215,\n",
       " 0.06706674,\n",
       " 0.067090906,\n",
       " 0.06722579,\n",
       " 0.06730925,\n",
       " 0.06733718,\n",
       " 0.06759968,\n",
       " 0.06761226,\n",
       " 0.067674905,\n",
       " 0.06791605,\n",
       " 0.068019785,\n",
       " 0.06809995,\n",
       " 0.06821226,\n",
       " 0.06829308,\n",
       " 0.06839197,\n",
       " 0.06842267,\n",
       " 0.06844009,\n",
       " 0.06856106,\n",
       " 0.06858277,\n",
       " 0.06886005,\n",
       " 0.06891051,\n",
       " 0.06891747,\n",
       " 0.06930799,\n",
       " 0.069378495,\n",
       " 0.06950836,\n",
       " 0.069533676,\n",
       " 0.069616556,\n",
       " 0.069844455,\n",
       " 0.06987599,\n",
       " 0.069924384,\n",
       " 0.07002174,\n",
       " 0.07008201,\n",
       " 0.07036944,\n",
       " 0.070624985,\n",
       " 0.07072996,\n",
       " 0.070983976,\n",
       " 0.07099129,\n",
       " 0.07109983,\n",
       " 0.071150735,\n",
       " 0.07123694,\n",
       " 0.07129063,\n",
       " 0.071297534,\n",
       " 0.071412764,\n",
       " 0.07149465,\n",
       " 0.07164357,\n",
       " 0.071696825,\n",
       " 0.07201916,\n",
       " 0.072033994,\n",
       " 0.07210003,\n",
       " 0.07213083,\n",
       " 0.072207026,\n",
       " 0.07235972,\n",
       " 0.07239116,\n",
       " 0.07243271,\n",
       " 0.07251034,\n",
       " 0.07276413,\n",
       " 0.07291316,\n",
       " 0.072942466,\n",
       " 0.073307864,\n",
       " 0.07341534,\n",
       " 0.07345072,\n",
       " 0.073602006,\n",
       " 0.073847786,\n",
       " 0.07392132,\n",
       " 0.07411428,\n",
       " 0.07420823,\n",
       " 0.07442048,\n",
       " 0.07453385,\n",
       " 0.074567,\n",
       " 0.07459666,\n",
       " 0.07462242,\n",
       " 0.074805886,\n",
       " 0.07489872,\n",
       " 0.07497726,\n",
       " 0.07499791,\n",
       " 0.0750648,\n",
       " 0.07523075,\n",
       " 0.07529179,\n",
       " 0.07551305,\n",
       " 0.07551697,\n",
       " 0.07555842,\n",
       " 0.075639695,\n",
       " 0.075660735,\n",
       " 0.07579868,\n",
       " 0.07596868,\n",
       " 0.07599091,\n",
       " 0.07599764,\n",
       " 0.07613452,\n",
       " 0.07632064,\n",
       " 0.076504774,\n",
       " 0.0771211,\n",
       " 0.07729245,\n",
       " 0.07734002,\n",
       " 0.07746908,\n",
       " 0.07773884,\n",
       " 0.07795723,\n",
       " 0.07800895,\n",
       " 0.0781491,\n",
       " 0.07836016,\n",
       " 0.07849675,\n",
       " 0.0785068,\n",
       " 0.07867337,\n",
       " 0.0788862,\n",
       " 0.07890207,\n",
       " 0.078914605,\n",
       " 0.078954384,\n",
       " 0.07910295,\n",
       " 0.07917624,\n",
       " 0.07922989,\n",
       " 0.07929188,\n",
       " 0.07948997,\n",
       " 0.07950792,\n",
       " 0.079553604,\n",
       " 0.079619914,\n",
       " 0.079762965,\n",
       " 0.079900555,\n",
       " 0.07991262,\n",
       " 0.079967834,\n",
       " 0.080153026,\n",
       " 0.08016487,\n",
       " 0.08062913,\n",
       " 0.080649115,\n",
       " 0.08071907,\n",
       " 0.08098481,\n",
       " 0.080984846,\n",
       " 0.08114145,\n",
       " 0.08119764,\n",
       " 0.081303544,\n",
       " 0.08132643,\n",
       " 0.08133695,\n",
       " 0.08134587,\n",
       " 0.08139162,\n",
       " 0.0814431,\n",
       " 0.08147346,\n",
       " 0.08148891,\n",
       " 0.0815173,\n",
       " 0.08153235,\n",
       " 0.08164906,\n",
       " 0.08171259,\n",
       " 0.082012825,\n",
       " 0.08220513,\n",
       " 0.082221694,\n",
       " 0.082485825,\n",
       " 0.082831375,\n",
       " 0.08294741,\n",
       " 0.08315939,\n",
       " 0.08318006,\n",
       " 0.08335208,\n",
       " 0.08341248,\n",
       " 0.08355957,\n",
       " 0.08359572,\n",
       " 0.083602905,\n",
       " 0.08368333,\n",
       " 0.08373963,\n",
       " 0.083876945,\n",
       " 0.08406342,\n",
       " 0.0842578,\n",
       " 0.08430111,\n",
       " 0.084542684,\n",
       " 0.084703796,\n",
       " 0.08476281,\n",
       " 0.08476906,\n",
       " 0.084771276,\n",
       " 0.08482299,\n",
       " 0.085144415,\n",
       " 0.08525867,\n",
       " 0.08527955,\n",
       " 0.08550213,\n",
       " 0.0855217,\n",
       " 0.0858995,\n",
       " 0.08600668,\n",
       " 0.086011596,\n",
       " 0.086220026,\n",
       " 0.08645576,\n",
       " 0.08672527,\n",
       " 0.086764894,\n",
       " 0.08679518,\n",
       " 0.086914085,\n",
       " 0.0870531,\n",
       " 0.08715382,\n",
       " 0.08724805,\n",
       " 0.08730696,\n",
       " 0.08730766,\n",
       " 0.08733968,\n",
       " 0.087727375,\n",
       " 0.08780565,\n",
       " 0.08806608,\n",
       " 0.0881435,\n",
       " 0.08822166,\n",
       " 0.08824047,\n",
       " 0.08890703,\n",
       " 0.08898785,\n",
       " 0.0890076,\n",
       " 0.08922696,\n",
       " 0.08924799,\n",
       " 0.08929092,\n",
       " 0.08949785,\n",
       " 0.089810975,\n",
       " 0.08981178,\n",
       " 0.089820884,\n",
       " 0.08982236,\n",
       " 0.08994598,\n",
       " 0.090045914,\n",
       " 0.0902236,\n",
       " 0.09069598,\n",
       " 0.09078797,\n",
       " 0.0908066,\n",
       " 0.09081219,\n",
       " 0.090981275,\n",
       " 0.09100067,\n",
       " 0.09121201,\n",
       " 0.09126012,\n",
       " 0.09136293,\n",
       " 0.09162736,\n",
       " 0.09164608,\n",
       " 0.09165934,\n",
       " 0.09187757,\n",
       " 0.09190833,\n",
       " 0.092129186,\n",
       " 0.09218577,\n",
       " 0.092575684,\n",
       " 0.092895634,\n",
       " 0.093388826,\n",
       " 0.093480915,\n",
       " 0.09358136,\n",
       " 0.09375311,\n",
       " 0.09402912,\n",
       " 0.094118685,\n",
       " 0.09421383,\n",
       " 0.094245724,\n",
       " 0.09436094,\n",
       " 0.09449008,\n",
       " 0.09487919,\n",
       " 0.0949546,\n",
       " 0.09508086,\n",
       " 0.09508571,\n",
       " 0.09514949,\n",
       " 0.095241904,\n",
       " 0.095446244,\n",
       " 0.09545867,\n",
       " 0.095463626,\n",
       " 0.095492154,\n",
       " 0.09554614,\n",
       " 0.09564456,\n",
       " 0.095829636,\n",
       " 0.096144944,\n",
       " 0.09617707,\n",
       " 0.096474,\n",
       " 0.096570075,\n",
       " 0.096689425,\n",
       " 0.09685522,\n",
       " 0.09685543,\n",
       " 0.09694345,\n",
       " 0.09698708,\n",
       " 0.097010426,\n",
       " 0.09706702,\n",
       " 0.09708318,\n",
       " 0.09724642,\n",
       " 0.09748122,\n",
       " 0.097633235,\n",
       " 0.09823438,\n",
       " 0.09836503,\n",
       " 0.09855291,\n",
       " 0.098653615,\n",
       " 0.09885392,\n",
       " 0.0990814,\n",
       " 0.09931347,\n",
       " 0.09971638,\n",
       " 0.09984323,\n",
       " 0.10010007,\n",
       " 0.100263976,\n",
       " 0.10041386,\n",
       " 0.10083439,\n",
       " 0.100849055,\n",
       " 0.10153923,\n",
       " 0.10189756,\n",
       " 0.1019453,\n",
       " 0.10218899,\n",
       " 0.10219034,\n",
       " 0.10237825,\n",
       " 0.10245239,\n",
       " 0.10266023,\n",
       " 0.102682956,\n",
       " 0.10274003,\n",
       " 0.10284737,\n",
       " 0.10292483,\n",
       " 0.10299191,\n",
       " 0.1030137,\n",
       " 0.10322017,\n",
       " 0.103285074,\n",
       " 0.10333653,\n",
       " 0.10335383,\n",
       " 0.10364609,\n",
       " 0.10374513,\n",
       " 0.10377062,\n",
       " 0.10398405,\n",
       " 0.10400388,\n",
       " 0.1041685,\n",
       " 0.10421293,\n",
       " 0.10424566,\n",
       " 0.10427475,\n",
       " 0.10468703,\n",
       " 0.10474141,\n",
       " 0.10484491,\n",
       " 0.10484683,\n",
       " 0.10493002,\n",
       " 0.10495039,\n",
       " 0.10496043,\n",
       " 0.105321296,\n",
       " 0.10541999,\n",
       " 0.1055031,\n",
       " 0.10589055,\n",
       " 0.106047146,\n",
       " 0.106285416,\n",
       " 0.106341675,\n",
       " 0.10653115,\n",
       " 0.106684215,\n",
       " 0.10672697,\n",
       " 0.106919155,\n",
       " 0.10709223,\n",
       " 0.107447274,\n",
       " 0.10773644,\n",
       " 0.10780689,\n",
       " 0.10799132,\n",
       " 0.108024925,\n",
       " 0.10814268,\n",
       " 0.10822253,\n",
       " 0.10828802,\n",
       " 0.10831076,\n",
       " 0.10833546,\n",
       " 0.108412914,\n",
       " 0.10879781,\n",
       " 0.10882344,\n",
       " 0.1091038,\n",
       " 0.10929058,\n",
       " 0.1095859,\n",
       " 0.10990662,\n",
       " 0.110072486,\n",
       " 0.11011162,\n",
       " 0.110145874,\n",
       " 0.11037494,\n",
       " 0.11053489,\n",
       " 0.11053722,\n",
       " 0.110700116,\n",
       " 0.110804684,\n",
       " 0.11080871,\n",
       " 0.111052655,\n",
       " 0.11124939,\n",
       " 0.11138097,\n",
       " 0.11158912,\n",
       " 0.11189131,\n",
       " 0.11197457,\n",
       " 0.11201416,\n",
       " 0.112109885,\n",
       " 0.112280466,\n",
       " 0.112393446,\n",
       " 0.11246569,\n",
       " 0.11250762,\n",
       " 0.11263506,\n",
       " 0.11276431,\n",
       " 0.11319573,\n",
       " 0.11325112,\n",
       " 0.11383564,\n",
       " 0.113848686,\n",
       " 0.113860935,\n",
       " 0.11391139,\n",
       " 0.1140357,\n",
       " 0.11423889,\n",
       " 0.11430956,\n",
       " 0.11449794,\n",
       " 0.114506915,\n",
       " 0.11499042,\n",
       " 0.11500041,\n",
       " 0.115027815,\n",
       " 0.115029626,\n",
       " 0.11533044,\n",
       " 0.1153322,\n",
       " 0.1155223,\n",
       " 0.11561458,\n",
       " 0.115719795,\n",
       " 0.115826026,\n",
       " 0.11617884,\n",
       " 0.11629284,\n",
       " 0.116341166,\n",
       " 0.1164897,\n",
       " 0.11661002,\n",
       " 0.11679119,\n",
       " 0.11697173,\n",
       " 0.11721524,\n",
       " 0.11724606,\n",
       " 0.117408626,\n",
       " 0.11742183,\n",
       " 0.11744016,\n",
       " 0.11769441,\n",
       " 0.117989406,\n",
       " 0.118003406,\n",
       " 0.11897162,\n",
       " 0.11918105,\n",
       " 0.11949558,\n",
       " 0.11951014,\n",
       " 0.11964505,\n",
       " 0.12011274,\n",
       " 0.12015788,\n",
       " 0.1202416,\n",
       " 0.12035974,\n",
       " 0.12044611,\n",
       " 0.12082657,\n",
       " 0.1208678,\n",
       " 0.121409826,\n",
       " 0.12158821,\n",
       " 0.121639214,\n",
       " 0.12166814,\n",
       " 0.12169467,\n",
       " 0.121788144,\n",
       " 0.121921144,\n",
       " 0.12229851,\n",
       " 0.12338889,\n",
       " 0.123527855,\n",
       " 0.12370156,\n",
       " 0.12443507,\n",
       " 0.124440566,\n",
       " 0.124569364,\n",
       " 0.1245726,\n",
       " 0.1247725,\n",
       " 0.124775216,\n",
       " 0.12565158,\n",
       " 0.12610056,\n",
       " 0.12620908,\n",
       " 0.1262162,\n",
       " 0.12681335,\n",
       " 0.1270487,\n",
       " 0.12741521,\n",
       " 0.12780714,\n",
       " 0.12780945,\n",
       " 0.12796497,\n",
       " 0.12838432,\n",
       " 0.12846279,\n",
       " 0.12847587,\n",
       " 0.12877366,\n",
       " 0.12883522,\n",
       " 0.12921442,\n",
       " 0.12953205,\n",
       " 0.13037835,\n",
       " 0.13108158,\n",
       " 0.13112484,\n",
       " 0.13149759,\n",
       " 0.13150564,\n",
       " 0.13152489,\n",
       " 0.13155244,\n",
       " 0.13174926,\n",
       " 0.13225271,\n",
       " 0.132426,\n",
       " 0.13255101,\n",
       " 0.13268611,\n",
       " 0.13270973,\n",
       " 0.13295124,\n",
       " 0.13306189,\n",
       " 0.13383336,\n",
       " 0.13396165,\n",
       " 0.13410741,\n",
       " 0.13420583,\n",
       " 0.13431212,\n",
       " 0.13432057,\n",
       " 0.13473131,\n",
       " 0.13483353,\n",
       " 0.13524729,\n",
       " 0.13529257,\n",
       " 0.13559021,\n",
       " 0.13592523,\n",
       " 0.13615468,\n",
       " 0.13624139,\n",
       " 0.13694148,\n",
       " 0.13751194,\n",
       " 0.1379733,\n",
       " 0.13804446,\n",
       " 0.13827601,\n",
       " 0.13876465,\n",
       " 0.13885377,\n",
       " 0.13990189,\n",
       " 0.1406545,\n",
       " 0.1409996,\n",
       " 0.14112271,\n",
       " 0.14127456,\n",
       " 0.14146914,\n",
       " 0.141501,\n",
       " 0.14170438,\n",
       " 0.14184126,\n",
       " 0.14230959,\n",
       " 0.14237973,\n",
       " 0.14258677,\n",
       " 0.1433813,\n",
       " 0.14361502,\n",
       " 0.14413516,\n",
       " 0.14452279,\n",
       " 0.14498791,\n",
       " 0.14524958,\n",
       " 0.14526422,\n",
       " 0.14735796,\n",
       " 0.14737567,\n",
       " 0.14961615,\n",
       " 0.15028173,\n",
       " 0.15083931,\n",
       " 0.15151344,\n",
       " 0.15195009,\n",
       " 0.15268938,\n",
       " 0.15303789,\n",
       " 0.15329431,\n",
       " 0.15336448,\n",
       " 0.15355651,\n",
       " 0.15359768,\n",
       " 0.15417637,\n",
       " 0.15422551,\n",
       " 0.15507999,\n",
       " 0.1560907,\n",
       " 0.15690337,\n",
       " 0.15713026,\n",
       " 0.15760744,\n",
       " 0.15811156,\n",
       " 0.15929133,\n",
       " 0.16092661,\n",
       " 0.16260011,\n",
       " 0.16325408,\n",
       " 0.16349508,\n",
       " 0.16359721,\n",
       " 0.16504708,\n",
       " 0.16527636,\n",
       " 0.16542962,\n",
       " 0.16628917,\n",
       " 0.16723387,\n",
       " 0.1673248,\n",
       " 0.16765907,\n",
       " 0.16779651,\n",
       " 0.16786592,\n",
       " 0.16900621,\n",
       " 0.16911581,\n",
       " 0.16918755,\n",
       " 0.16934365,\n",
       " 0.16936798,\n",
       " 0.17461155,\n",
       " 0.17588368,\n",
       " 0.17645991,\n",
       " 0.176828,\n",
       " 0.17829409,\n",
       " 0.18345445,\n",
       " 0.18347137,\n",
       " 0.18360567,\n",
       " 0.18387024,\n",
       " 0.18426976,\n",
       " 0.18448232,\n",
       " 0.18462913,\n",
       " 0.18659197,\n",
       " 0.18801308,\n",
       " 0.18858936,\n",
       " 0.1886735,\n",
       " 0.18882152,\n",
       " 0.19242427,\n",
       " 0.19628401,\n",
       " 0.19635479,\n",
       " 0.20234159,\n",
       " 0.2107423,\n",
       " 0.21089199,\n",
       " 0.21132706,\n",
       " 0.21797219,\n",
       " 0.22324212,\n",
       " 0.23086187,\n",
       " 0.23592818,\n",
       " 0.23940447,\n",
       " 0.24103013,\n",
       " 0.2465385,\n",
       " 0.2691094,\n",
       " 0.38495764]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([np.mean(a) for a in mean_error_ml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100., 311., 249., 103.,  39.,   7.,   5.,   0.,   0.,   1.]),\n",
       " array([0.01050521, 0.04795045, 0.08539569, 0.12284094, 0.16028619,\n",
       "        0.19773142, 0.23517667, 0.2726219 , 0.31006715, 0.3475124 ,\n",
       "        0.38495764], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([np.mean(a) for a in mean_error_ml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100., 310., 250., 103.,  39.,   7.,   5.,   0.,   0.,   1.]),\n",
       " array([0.01049461, 0.04795014, 0.08540567, 0.1228612 , 0.16031672,\n",
       "        0.19777225, 0.23522778, 0.27268331, 0.31013883, 0.34759436,\n",
       "        0.38504989]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([np.mean(a) for a in mean_error_dft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "447*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,) (17,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cc44b823cfd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_error_ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/cgcnn/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgcnn/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,) (17,) "
     ]
    }
   ],
   "source": [
    "np.mean(mean_error_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zulissi/miniconda3/envs/cgcnn/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/home/zulissi/miniconda3/envs/cgcnn/lib/python3.7/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0,'/home/junwoony/.local/lib/python3.6/site-packages')\n",
    "# sys.path.insert(0,'/home/zulissi/software/adamwr')\n",
    "import numpy as np\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import mongo\n",
    "# from torchviz import make_dot, make_dot_from_trace\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mongo\n",
    "from cgcnn.data_grad import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import multiprocess as mp\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache cached_docs.pkl docs SDT_list  orig_atom_fea_len nbr_fea_len\n",
    "docs = pickle.load(open('./input/H_docs.pkl', 'rb'))\n",
    "\n",
    "docs = [doc for doc in docs if doc['coordination']==doc['initial_coordination'] and doc['coordination']!=''\n",
    "                               and doc['movement_data']['max_adsorbate_movement']<1.0\n",
    "                               and doc['movement_data']['max_slab_movement']<1.0\n",
    "                               and 'N' not in doc['atoms']['chemical_symbols'] #no nitrides\n",
    "                               and 'C' not in doc['atoms']['chemical_symbols'] #no carbides\n",
    "                               and 'S' not in doc['atoms']['chemical_symbols'] #no sulfides\n",
    "                               and 'Se' not in doc['atoms']['chemical_symbols'] #no selenides\n",
    "                               and doc['atoms']['symbol_counts']['H']<2 #no hydrides\n",
    "        ]\n",
    "random.shuffle(docs)\n",
    "docs = docs[0:10000]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "SDT = StructureDataTransformer(atom_init_loc='atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step= 0.2,\n",
    "                              radius=12,\n",
    "                              use_tag=True,\n",
    "                              use_fixed_info=True,\n",
    "                              bond_property=True,\n",
    "#                               use_voronoi=False,\n",
    "#                               train_geometry = 'initial',\n",
    "                              is_initial=True\n",
    "                              )\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "#     SDT_list = list(tqdm.tqdm(map(lambda x: SDT_out[x],range(len(SDT_out))),total=len(SDT_out)))\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "\n",
    "structures = SDT_list[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "SDT_time = end-start\n",
    "print('SDT conversion time', SDT_time)\n",
    "print('SDT list created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = np.array([sdt[-1].numpy() for sdt in SDT_list]).reshape(-1,1)\n",
    "# pickle.dump(SDT_list, open('./input/SDT_20000_grad.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "\n",
    "from cgcnn.data_grad import collate_pool, MergeDataset\n",
    "from cgcnn.model_grad_2 import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SDT_list= SDT_list\n",
    "target_list = target_list\n",
    "\n",
    "indices = np.arange(len(SDT_list))\n",
    "SDT_training, SDT_test, target_training, target_test, train_idx, test_idx \\\n",
    "= train_test_split(SDT_list, target_list, indices, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get an idea for what the DFT initial - final structure difference is; the ML models needs to beat this number for the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(target,sdt):\n",
    "    free_atom_idx = list(set(list(range(len(target[0]))))-set(sdt[-2].numpy()))\n",
    "    diff = np.sum(((target[0] - sdt[4].numpy())[free_atom_idx])**2.,axis=1)**0.5\n",
    "    return diff\n",
    "\n",
    "np.mean(np.abs(np.concatenate([diff(target,sdt) for sdt,target in zip(SDT_test, target_test)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamwr.adamw import AdamW\n",
    "from torch.optim.lbfgs import LBFGS\n",
    "\n",
    "from adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "\n",
    "# warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=100, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "\n",
    "#############\n",
    "# To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):        \n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        return torch.mean(torch.sqrt(torch.sum((y_pred-y_true.cuda())**2.0,dim=1)))\n",
    "#         return super().get_loss(y_pred, y_true, **kwargs)\n",
    "## return features = net.forward(SDT_test)\n",
    "############\n",
    "\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=100, #214\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs= 1000, \n",
    "    module__atom_fea_len=46, #46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8, #8\n",
    "    module__n_h=8,\n",
    "        module__opt_step_size=0.3,\n",
    "    module__min_opt_steps=20,\n",
    "    module__max_opt_steps=300,\n",
    "    module__steps=4,\n",
    "    optimizer__weight_decay=1e-3,\n",
    "    optimizer=AdamW,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, LR_schedule, load_best_valid_loss] #    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.module_.max_opt_steps=300\n",
    "net.module_.max_epochs=1000\n",
    "net.partial_fit(SDT_training, target_training)\n",
    "# net.module_.min_opt_steps=300\n",
    "# net.module_.max_epochs=10\n",
    "# net.partial_fit(SDT_training, target_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ase.io\n",
    "mean_error_ml = []\n",
    "mean_error_dft = []\n",
    "\n",
    "with ase.io.trajectory.TrajectoryWriter('test.traj') as traj:\n",
    "    for i, (doc, sdt) in enumerate(zip(np.array(docs)[test_idx], SDT_test)):\n",
    "        atoms_final = mongo.make_atoms_from_doc(doc)\n",
    "        final_pos = sdt[-1]\n",
    "        atoms_final.positions=final_pos\n",
    "        atoms_init = mongo.make_atoms_from_doc(doc['initial_configuration'])\n",
    "        ml_pos = list(net.forward_iter([sdt]))[0][-1].detach().numpy()\n",
    "        atoms_ml = atoms_final.copy()\n",
    "        atoms_ml.positions = ml_pos\n",
    "        traj.write(atoms_init)\n",
    "        traj.write(atoms_ml)\n",
    "        traj.write(atoms_final)\n",
    "        diff = np.abs(np.sqrt(np.sum((ml_pos-final_pos.numpy())**2.,axis=1)))\n",
    "        diff = diff[diff>1e-6]\n",
    "        \n",
    "        mean_error_ml.append(np.mean(np.abs(diff)))\n",
    "        \n",
    "        diff = np.abs(np.sqrt(np.sum((atoms_init.positions-final_pos.numpy())**2.,axis=1)))\n",
    "        diff = diff[diff>1e-6]\n",
    "        mean_error_dft.append(np.abs(diff))\n",
    "\n",
    "mean_error_ml = np.array(mean_error_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mean_error_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
